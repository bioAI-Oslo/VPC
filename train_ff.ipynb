{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a996003-8a02-4780-b283-614a14d4ee21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "\n",
    "from models import VPC_FF\n",
    "from train_tools import get_datasets, Logger, euclid\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3121abe-ee14-42c0-aaa9-9bb10fa1b58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"FF\"\n",
    "path = f\"./VPC\"\n",
    "spec_file = f\"{path}/model_parameters.json\"\n",
    "with open(spec_file, \"r\") as f:\n",
    "    params = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64322275-3f30-4fe8-9894-580e38b44903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Parameters: \n",
      " {\n",
      "     \"epochs\": 100,\n",
      "     \"batch_size\": 64,\n",
      "     \"lr\": 0.0001,\n",
      "     \"al1\": 10.0,\n",
      "     \"l2\": 0,\n",
      "     \"basis_vectors\": 500,\n",
      "     \"nodes\": 500,\n",
      "     \"outputs\": 100,\n",
      "     \"reset_interval\": 1,\n",
      "     \"context\": true\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(\"Model Parameters: \\n\", json.dumps(params, indent = 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48bf6299-7702-46b0-97c2-2578049b033d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VPC_FF(params)\n",
    "logger = Logger(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe9f5046-7f9c-469f-9ced-ee80929b9469",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▉                                                                                             | 2/100 [00:08<07:00,  4.29s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m (x_train, y_train)\n\u001b[1;32m      9\u001b[0m     loss, yhat, g \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtrain_step(inputs, y_train)\n\u001b[0;32m---> 10\u001b[0m     train_metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     train_metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meuclid\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m euclid(y_train, yhat)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     13\u001b[0m train_metrics \u001b[38;5;241m=\u001b[39m {key:train_metrics[key]\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m train_metrics}\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loader, val_loader = get_datasets(\"datasets/points\", context = params[\"context\"], device = model.device,\n",
    "                                        trajectories = False, batch_size = params[\"batch_size\"])\n",
    "\n",
    "for epoch in tqdm(range(params[\"epochs\"])):\n",
    "    # train step\n",
    "    train_metrics = {\"loss\":0, \"euclid\":0}\n",
    "    for i, (x_train, y_train) in enumerate(train_loader):\n",
    "        inputs = (x_train, y_train)\n",
    "        loss, yhat, g = model.train_step(inputs, y_train)\n",
    "        train_metrics[\"loss\"] += loss.item()\n",
    "        train_metrics[\"euclid\"] += euclid(y_train, yhat).item()\n",
    "    \n",
    "    train_metrics = {key:train_metrics[key]/len(train_loader) for key in train_metrics}\n",
    "    logger(train_metrics, \"train\")\n",
    "    \n",
    "    # validation step\n",
    "    val_metrics = {\"loss\":0, \"euclid\":0}\n",
    "    for j, (x_val, y_val) in enumerate(val_loader):\n",
    "        inputs = (x_val, y_val)\n",
    "        loss, yhat, g = model.val_step(inputs, y_val)\n",
    "        val_metrics[\"loss\"] += loss.item()\n",
    "        val_metrics[\"euclid\"] += euclid(y_val, yhat).item()\n",
    "        \n",
    "    val_metrics = {key:val_metrics[key]/len(val_loader) for key in val_metrics}\n",
    "    logger(val_metrics, \"val\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f69e472-85ed-4ee5-bc51-e92533a3668f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, f\"{path}/trained_{model_type}_model\")\n",
    "logger.save_metrics(name = model_type)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
